{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ea761f",
   "metadata": {},
   "source": [
    "# Improving Customer Experience of British Airways - Business Objective #3 \n",
    "\n",
    "Business objective #3 for improving customer experience of British Airways is as follows - analysing review topics for the four different seat types for a more targeted marketing approach, improving business by 10%.\n",
    "\n",
    "In this part of the project, topic modelling of customer reviews of their flight experiences will be done to sieve up areas of concern of customers to recommend to the airlines to better those aspects for a better flight experience for their customers. This would be done separately for the four groups - (1) economy, (2) premium economy, (3) business class and (4) first class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce5a892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Carlin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Carlin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing relevant libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings as wr\n",
    "import re\n",
    "\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "wr.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c4a60d",
   "metadata": {},
   "source": [
    "# Reading in source files from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c270d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OverallRating</th>\n",
       "      <th>ReviewHeader</th>\n",
       "      <th>Name</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>VerifiedReview</th>\n",
       "      <th>ReviewBody</th>\n",
       "      <th>TypeOfTraveller</th>\n",
       "      <th>SeatType</th>\n",
       "      <th>Route</th>\n",
       "      <th>DateFlown</th>\n",
       "      <th>...</th>\n",
       "      <th>Food&amp;Beverages</th>\n",
       "      <th>InflightEntertainment</th>\n",
       "      <th>Wifi&amp;Connectivity</th>\n",
       "      <th>CleanText</th>\n",
       "      <th>SpecialCharacters</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>HyphenatedWords</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Bigrams</th>\n",
       "      <th>Trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Service level far worse then Ryanair\"</td>\n",
       "      <td>L Keele</td>\n",
       "      <td>2023-11-19</td>\n",
       "      <td>True</td>\n",
       "      <td>4 Hours before takeoff we received a Mail stat...</td>\n",
       "      <td>Couple Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>London to Stuttgart</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hour takeoff received mail state cryptic messa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['hour', 'takeoff', 'received', 'mail', 'state...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.045588</td>\n",
       "      <td>[('hour', 'takeoff'), ('takeoff', 'received'),...</td>\n",
       "      <td>[('hour', 'takeoff', 'received'), ('takeoff', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>\"do not upgrade members based on status\"</td>\n",
       "      <td>Austin Jones</td>\n",
       "      <td>2023-11-19</td>\n",
       "      <td>True</td>\n",
       "      <td>I recently had a delay on British Airways from...</td>\n",
       "      <td>Business</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>Brussels to London</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>recently delay bru lhr due staff shortage anno...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['recently', 'delay', 'bru', 'lhr', 'due', 'st...</td>\n",
       "      <td>['world-class']</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>[('recently', 'delay'), ('delay', 'bru'), ('br...</td>\n",
       "      <td>[('recently', 'delay', 'bru'), ('delay', 'bru'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>\"Flight was smooth and quick\"</td>\n",
       "      <td>M A Collie</td>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>False</td>\n",
       "      <td>Boarded on time, but it took ages to get to th...</td>\n",
       "      <td>Couple Leisure</td>\n",
       "      <td>Business Class</td>\n",
       "      <td>London Heathrow to Dublin</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>boarded time take age get runway due congestio...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['boarded', 'time', 'take', 'age', 'get', 'run...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>[('boarded', 'time'), ('time', 'take'), ('take...</td>\n",
       "      <td>[('boarded', 'time', 'take'), ('time', 'take',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Absolutely hopeless airline\"</td>\n",
       "      <td>Nigel Dean</td>\n",
       "      <td>2023-11-16</td>\n",
       "      <td>True</td>\n",
       "      <td>5 days before the flight, we were advised by B...</td>\n",
       "      <td>Couple Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>London to Dublin</td>\n",
       "      <td>2022-12-01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>day advise cancelled ask rebook hour hour orig...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['day', 'advise', 'cancelled', 'ask', 'rebook'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.113889</td>\n",
       "      <td>[('day', 'advise'), ('advise', 'cancelled'), (...</td>\n",
       "      <td>[('day', 'advise', 'cancelled'), ('advise', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>\"Customer Service is non existent\"</td>\n",
       "      <td>Gaylynne Simpson</td>\n",
       "      <td>2023-11-14</td>\n",
       "      <td>False</td>\n",
       "      <td>We traveled to Lisbon for our dream vacation, ...</td>\n",
       "      <td>Couple Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>London to Lisbon</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>travel lisbon dream vacation cruise portugal s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['travel', 'lisbon', 'dream', 'vacation', 'cru...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.073958</td>\n",
       "      <td>[('travel', 'lisbon'), ('lisbon', 'dream'), ('...</td>\n",
       "      <td>[('travel', 'lisbon', 'dream'), ('lisbon', 'dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>\"I can’t imagine a worst airline\"</td>\n",
       "      <td>A Narden</td>\n",
       "      <td>2023-11-12</td>\n",
       "      <td>True</td>\n",
       "      <td>Booked a flight from Bucharest to Manchester w...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>Bucharest to Manchester via London</td>\n",
       "      <td>2023-09-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>book bucharest manchester layover heathrow del...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['book', 'bucharest', 'manchester', 'layover',...</td>\n",
       "      <td>[]</td>\n",
       "      <td>-0.243958</td>\n",
       "      <td>[('book', 'bucharest'), ('bucharest', 'manches...</td>\n",
       "      <td>[('book', 'bucharest', 'manchester'), ('buchar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8.0</td>\n",
       "      <td>\"sufficient leg and arm room\"</td>\n",
       "      <td>Graeme Boothman</td>\n",
       "      <td>2023-11-08</td>\n",
       "      <td>True</td>\n",
       "      <td>Booked online months ago and the only hitch wa...</td>\n",
       "      <td>Couple Leisure</td>\n",
       "      <td>Premium Economy</td>\n",
       "      <td>Manchester to Cape Town via London</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>book online month ago hitch replacement meanin...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['book', 'online', 'month', 'ago', 'hitch', 'r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.080565</td>\n",
       "      <td>[('book', 'online'), ('online', 'month'), ('mo...</td>\n",
       "      <td>[('book', 'online', 'month'), ('online', 'mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>“crew were polite”</td>\n",
       "      <td>R Vines</td>\n",
       "      <td>2023-11-07</td>\n",
       "      <td>True</td>\n",
       "      <td>The flight was on time. The crew were polite. ...</td>\n",
       "      <td>Solo Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>Seville to London Gatwick</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>time crew polite story outward europe generall...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['time', 'crew', 'polite', 'story', 'outward',...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>[('time', 'crew'), ('crew', 'polite'), ('polit...</td>\n",
       "      <td>[('time', 'crew', 'polite'), ('crew', 'polite'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.0</td>\n",
       "      <td>\"Angry, disappointed, and unsatisfied\"</td>\n",
       "      <td>Massimo Tricca</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>False</td>\n",
       "      <td>Angry, disappointed, and unsatisfied. My route...</td>\n",
       "      <td>Family Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>London Heatrow to Atlanta</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>angry disappoint unsatisfied route london atla...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['angry', 'disappoint', 'unsatisfied', 'route'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>[('angry', 'disappoint'), ('disappoint', 'unsa...</td>\n",
       "      <td>[('angry', 'disappoint', 'unsatisfied'), ('dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>\"BA now stands for Basic Airways\"</td>\n",
       "      <td>J Kaye</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>True</td>\n",
       "      <td>As an infrequent flyer, British Airways was al...</td>\n",
       "      <td>Couple Leisure</td>\n",
       "      <td>Economy Class</td>\n",
       "      <td>Gatwick to Antalya</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>infrequent flyer always first choice reassuran...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['infrequent', 'flyer', 'always', 'first', 'ch...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>[('infrequent', 'flyer'), ('flyer', 'always'),...</td>\n",
       "      <td>[('infrequent', 'flyer', 'always'), ('flyer', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OverallRating                              ReviewHeader              Name  \\\n",
       "0            1.0    \"Service level far worse then Ryanair\"           L Keele   \n",
       "1            3.0  \"do not upgrade members based on status\"      Austin Jones   \n",
       "2            8.0             \"Flight was smooth and quick\"        M A Collie   \n",
       "3            1.0             \"Absolutely hopeless airline\"        Nigel Dean   \n",
       "4            1.0        \"Customer Service is non existent\"  Gaylynne Simpson   \n",
       "5            1.0         \"I can’t imagine a worst airline\"          A Narden   \n",
       "6            8.0             \"sufficient leg and arm room\"   Graeme Boothman   \n",
       "7            7.0                        “crew were polite”           R Vines   \n",
       "8            2.0    \"Angry, disappointed, and unsatisfied\"    Massimo Tricca   \n",
       "9            3.0         \"BA now stands for Basic Airways\"            J Kaye   \n",
       "\n",
       "     Datetime  VerifiedReview  \\\n",
       "0  2023-11-19            True   \n",
       "1  2023-11-19            True   \n",
       "2  2023-11-16           False   \n",
       "3  2023-11-16            True   \n",
       "4  2023-11-14           False   \n",
       "5  2023-11-12            True   \n",
       "6  2023-11-08            True   \n",
       "7  2023-11-07            True   \n",
       "8  2023-11-05           False   \n",
       "9  2023-11-05            True   \n",
       "\n",
       "                                          ReviewBody TypeOfTraveller  \\\n",
       "0  4 Hours before takeoff we received a Mail stat...  Couple Leisure   \n",
       "1  I recently had a delay on British Airways from...        Business   \n",
       "2  Boarded on time, but it took ages to get to th...  Couple Leisure   \n",
       "3  5 days before the flight, we were advised by B...  Couple Leisure   \n",
       "4  We traveled to Lisbon for our dream vacation, ...  Couple Leisure   \n",
       "5  Booked a flight from Bucharest to Manchester w...    Solo Leisure   \n",
       "6  Booked online months ago and the only hitch wa...  Couple Leisure   \n",
       "7  The flight was on time. The crew were polite. ...    Solo Leisure   \n",
       "8  Angry, disappointed, and unsatisfied. My route...  Family Leisure   \n",
       "9  As an infrequent flyer, British Airways was al...  Couple Leisure   \n",
       "\n",
       "          SeatType                               Route   DateFlown  ...  \\\n",
       "0    Economy Class                 London to Stuttgart  2023-11-01  ...   \n",
       "1    Economy Class                  Brussels to London  2023-11-01  ...   \n",
       "2   Business Class           London Heathrow to Dublin  2023-11-01  ...   \n",
       "3    Economy Class                    London to Dublin  2022-12-01  ...   \n",
       "4    Economy Class                    London to Lisbon  2023-11-01  ...   \n",
       "5    Economy Class  Bucharest to Manchester via London  2023-09-01  ...   \n",
       "6  Premium Economy  Manchester to Cape Town via London  2023-11-01  ...   \n",
       "7    Economy Class          Seville to London Gatwick   2023-11-01  ...   \n",
       "8    Economy Class           London Heatrow to Atlanta  2023-11-01  ...   \n",
       "9    Economy Class                  Gatwick to Antalya  2023-10-01  ...   \n",
       "\n",
       "   Food&Beverages  InflightEntertainment  Wifi&Connectivity  \\\n",
       "0             NaN                    NaN                NaN   \n",
       "1             1.0                    2.0                2.0   \n",
       "2             4.0                    NaN                NaN   \n",
       "3             NaN                    NaN                NaN   \n",
       "4             1.0                    1.0                1.0   \n",
       "5             1.0                    1.0                NaN   \n",
       "6             4.0                    4.0                NaN   \n",
       "7             3.0                    NaN                NaN   \n",
       "8             4.0                    4.0                3.0   \n",
       "9             1.0                    1.0                1.0   \n",
       "\n",
       "                                           CleanText SpecialCharacters  \\\n",
       "0  hour takeoff received mail state cryptic messa...                []   \n",
       "1  recently delay bru lhr due staff shortage anno...                []   \n",
       "2  boarded time take age get runway due congestio...                []   \n",
       "3  day advise cancelled ask rebook hour hour orig...                []   \n",
       "4  travel lisbon dream vacation cruise portugal s...                []   \n",
       "5  book bucharest manchester layover heathrow del...                []   \n",
       "6  book online month ago hitch replacement meanin...                []   \n",
       "7  time crew polite story outward europe generall...                []   \n",
       "8  angry disappoint unsatisfied route london atla...                []   \n",
       "9  infrequent flyer always first choice reassuran...                []   \n",
       "\n",
       "                                              Tokens  HyphenatedWords  \\\n",
       "0  ['hour', 'takeoff', 'received', 'mail', 'state...               []   \n",
       "1  ['recently', 'delay', 'bru', 'lhr', 'due', 'st...  ['world-class']   \n",
       "2  ['boarded', 'time', 'take', 'age', 'get', 'run...               []   \n",
       "3  ['day', 'advise', 'cancelled', 'ask', 'rebook'...               []   \n",
       "4  ['travel', 'lisbon', 'dream', 'vacation', 'cru...               []   \n",
       "5  ['book', 'bucharest', 'manchester', 'layover',...               []   \n",
       "6  ['book', 'online', 'month', 'ago', 'hitch', 'r...               []   \n",
       "7  ['time', 'crew', 'polite', 'story', 'outward',...               []   \n",
       "8  ['angry', 'disappoint', 'unsatisfied', 'route'...               []   \n",
       "9  ['infrequent', 'flyer', 'always', 'first', 'ch...               []   \n",
       "\n",
       "   Sentiment                                            Bigrams  \\\n",
       "0  -0.045588  [('hour', 'takeoff'), ('takeoff', 'received'),...   \n",
       "1   0.006944  [('recently', 'delay'), ('delay', 'bru'), ('br...   \n",
       "2   0.128030  [('boarded', 'time'), ('time', 'take'), ('take...   \n",
       "3   0.113889  [('day', 'advise'), ('advise', 'cancelled'), (...   \n",
       "4  -0.073958  [('travel', 'lisbon'), ('lisbon', 'dream'), ('...   \n",
       "5  -0.243958  [('book', 'bucharest'), ('bucharest', 'manches...   \n",
       "6   0.080565  [('book', 'online'), ('online', 'month'), ('mo...   \n",
       "7   0.212500  [('time', 'crew'), ('crew', 'polite'), ('polit...   \n",
       "8   0.024176  [('angry', 'disappoint'), ('disappoint', 'unsa...   \n",
       "9   0.087719  [('infrequent', 'flyer'), ('flyer', 'always'),...   \n",
       "\n",
       "                                            Trigrams  \n",
       "0  [('hour', 'takeoff', 'received'), ('takeoff', ...  \n",
       "1  [('recently', 'delay', 'bru'), ('delay', 'bru'...  \n",
       "2  [('boarded', 'time', 'take'), ('time', 'take',...  \n",
       "3  [('day', 'advise', 'cancelled'), ('advise', 'c...  \n",
       "4  [('travel', 'lisbon', 'dream'), ('lisbon', 'dr...  \n",
       "5  [('book', 'bucharest', 'manchester'), ('buchar...  \n",
       "6  [('book', 'online', 'month'), ('online', 'mont...  \n",
       "7  [('time', 'crew', 'polite'), ('crew', 'polite'...  \n",
       "8  [('angry', 'disappoint', 'unsatisfied'), ('dis...  \n",
       "9  [('infrequent', 'flyer', 'always'), ('flyer', ...  \n",
       "\n",
       "[10 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in preprocessed_data.csv and display the first 5 lines of the data\n",
    "df = pd.read_csv(\"preprocessed_data.csv\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a867815",
   "metadata": {},
   "source": [
    "# 2. Data Preparation - Preprocessing \n",
    "\n",
    "\n",
    "In addition to the earlier cleaning as a group which included (1) Case converter - convert to lower case, (2) Punctuation erasure, (3) Number filter, (4) Stop word filter, (5) N-chars set to 3: removal of tokens less than 3 chars and (6) removal of specific POS through POS tagging, further customsised preprocessing is done for business objective 3.\n",
    "\n",
    "This will include (1) Cleaning of bigram and ngram columns for our analysis purposes, and (2) dropping of irrelevant columns and splitting the dataset by seat types (economy, premium economy, business class and first class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5b7b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns for analysis\n",
    "df_cleaned = df.drop(columns=['ReviewHeader', 'Datetime', 'ReviewBody', 'SpecialCharacters', 'HyphenatedWords', \n",
    "                              'OverallRating', 'Name', 'VerifiedReview', 'TypeOfTraveller', 'Route', 'DateFlown', \n",
    "                              'SeatComfort', 'CabinStaffService', 'GroundService', 'ValueForMoney', 'Aircraft', \n",
    "                              'Food&Beverages', 'InflightEntertainment', 'Wifi&Connectivity'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4aaf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           CleanText  \\\n",
      "0  hour takeoff received mail state cryptic messa...   \n",
      "1  recently delay bru lhr due staff shortage anno...   \n",
      "2  boarded time take age get runway due congestio...   \n",
      "3  day advise cancelled ask rebook hour hour orig...   \n",
      "4  travel lisbon dream vacation cruise portugal s...   \n",
      "\n",
      "                                          CleanText2  \n",
      "0  [hour, takeoff, received, mail, state, cryptic...  \n",
      "1  [recently, delay, bru, lhr, due, staff, shorta...  \n",
      "2  [boarded, time, take, age, get, runway, due, c...  \n",
      "3  [day, advise, cancelled, ask, rebook, hour, ho...  \n",
      "4  [travel, lisbon, dream, vacation, cruise, port...  \n"
     ]
    }
   ],
   "source": [
    "# Tokenize the CLeanText column for further analysis later and create another column named 'CleanText2'\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the 'CleanText' column and keep results in new columns 'CleanText2'\n",
    "df_cleaned['CleanText2'] = df_cleaned['CleanText'].apply(word_tokenize)\n",
    "\n",
    "# Display the first few rows to confirm the tokenized output\n",
    "print(df_cleaned[['CleanText', 'CleanText2']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8942ece4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           CleanText  \\\n",
      "0  hour takeoff received mail state cryptic messa...   \n",
      "1  recently delay bru lhr due staff shortage anno...   \n",
      "2  boarded time take age get runway due congestio...   \n",
      "3  day advise cancelled ask rebook hour hour orig...   \n",
      "4  travel lisbon dream vacation cruise portugal s...   \n",
      "\n",
      "                                          CleanText2  \n",
      "0  [hour, takeoff, received, mail, state, cryptic...  \n",
      "1  [recently, delay, bru, lhr, due, staff, shorta...  \n",
      "2  [boarded, time, take, age, get, runway, due, c...  \n",
      "3  [day, advise, cancelled, ask, rebook, hour, ho...  \n",
      "4  [travel, lisbon, dream, vacation, cruise, port...  \n"
     ]
    }
   ],
   "source": [
    "# Defining additional stop words\n",
    "additional_stop = {\"economy\", \"premium\", \"class\", \"first\", \"business\"}\n",
    "\n",
    "# Removing from the CleanText2 column\n",
    "df_cleaned['CleanText2'] = df_cleaned['CleanText2'].apply(lambda tokens: [word for word in tokens if word.lower() not in additional_stop])\n",
    "\n",
    "print(df_cleaned[['CleanText', 'CleanText2']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189a2d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeatType</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>CleanText</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Bigrams</th>\n",
       "      <th>Trigrams</th>\n",
       "      <th>CleanText2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>no</td>\n",
       "      <td>hour takeoff received mail state cryptic messa...</td>\n",
       "      <td>['hour', 'takeoff', 'received', 'mail', 'state...</td>\n",
       "      <td>-0.045588</td>\n",
       "      <td>[('hour', 'takeoff'), ('takeoff', 'received'),...</td>\n",
       "      <td>[('hour', 'takeoff', 'received'), ('takeoff', ...</td>\n",
       "      <td>[hour, takeoff, received, mail, state, cryptic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>no</td>\n",
       "      <td>recently delay bru lhr due staff shortage anno...</td>\n",
       "      <td>['recently', 'delay', 'bru', 'lhr', 'due', 'st...</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>[('recently', 'delay'), ('delay', 'bru'), ('br...</td>\n",
       "      <td>[('recently', 'delay', 'bru'), ('delay', 'bru'...</td>\n",
       "      <td>[recently, delay, bru, lhr, due, staff, shorta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business Class</td>\n",
       "      <td>yes</td>\n",
       "      <td>boarded time take age get runway due congestio...</td>\n",
       "      <td>['boarded', 'time', 'take', 'age', 'get', 'run...</td>\n",
       "      <td>0.128030</td>\n",
       "      <td>[('boarded', 'time'), ('time', 'take'), ('take...</td>\n",
       "      <td>[('boarded', 'time', 'take'), ('time', 'take',...</td>\n",
       "      <td>[boarded, time, take, age, get, runway, due, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>no</td>\n",
       "      <td>day advise cancelled ask rebook hour hour orig...</td>\n",
       "      <td>['day', 'advise', 'cancelled', 'ask', 'rebook'...</td>\n",
       "      <td>0.113889</td>\n",
       "      <td>[('day', 'advise'), ('advise', 'cancelled'), (...</td>\n",
       "      <td>[('day', 'advise', 'cancelled'), ('advise', 'c...</td>\n",
       "      <td>[day, advise, cancelled, ask, rebook, hour, ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>no</td>\n",
       "      <td>travel lisbon dream vacation cruise portugal s...</td>\n",
       "      <td>['travel', 'lisbon', 'dream', 'vacation', 'cru...</td>\n",
       "      <td>-0.073958</td>\n",
       "      <td>[('travel', 'lisbon'), ('lisbon', 'dream'), ('...</td>\n",
       "      <td>[('travel', 'lisbon', 'dream'), ('lisbon', 'dr...</td>\n",
       "      <td>[travel, lisbon, dream, vacation, cruise, port...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>no</td>\n",
       "      <td>book bucharest manchester layover heathrow del...</td>\n",
       "      <td>['book', 'bucharest', 'manchester', 'layover',...</td>\n",
       "      <td>-0.243958</td>\n",
       "      <td>[('book', 'bucharest'), ('bucharest', 'manches...</td>\n",
       "      <td>[('book', 'bucharest', 'manchester'), ('buchar...</td>\n",
       "      <td>[book, bucharest, manchester, layover, heathro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Premium Economy</td>\n",
       "      <td>yes</td>\n",
       "      <td>book online month ago hitch replacement meanin...</td>\n",
       "      <td>['book', 'online', 'month', 'ago', 'hitch', 'r...</td>\n",
       "      <td>0.080565</td>\n",
       "      <td>[('book', 'online'), ('online', 'month'), ('mo...</td>\n",
       "      <td>[('book', 'online', 'month'), ('online', 'mont...</td>\n",
       "      <td>[book, online, month, ago, hitch, replacement,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>yes</td>\n",
       "      <td>time crew polite story outward europe generall...</td>\n",
       "      <td>['time', 'crew', 'polite', 'story', 'outward',...</td>\n",
       "      <td>0.212500</td>\n",
       "      <td>[('time', 'crew'), ('crew', 'polite'), ('polit...</td>\n",
       "      <td>[('time', 'crew', 'polite'), ('crew', 'polite'...</td>\n",
       "      <td>[time, crew, polite, story, outward, europe, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>yes</td>\n",
       "      <td>angry disappoint unsatisfied route london atla...</td>\n",
       "      <td>['angry', 'disappoint', 'unsatisfied', 'route'...</td>\n",
       "      <td>0.024176</td>\n",
       "      <td>[('angry', 'disappoint'), ('disappoint', 'unsa...</td>\n",
       "      <td>[('angry', 'disappoint', 'unsatisfied'), ('dis...</td>\n",
       "      <td>[angry, disappoint, unsatisfied, route, london...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Economy Class</td>\n",
       "      <td>no</td>\n",
       "      <td>infrequent flyer always first choice reassuran...</td>\n",
       "      <td>['infrequent', 'flyer', 'always', 'first', 'ch...</td>\n",
       "      <td>0.087719</td>\n",
       "      <td>[('infrequent', 'flyer'), ('flyer', 'always'),...</td>\n",
       "      <td>[('infrequent', 'flyer', 'always'), ('flyer', ...</td>\n",
       "      <td>[infrequent, flyer, always, choice, reassuranc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          SeatType Recommended  \\\n",
       "0    Economy Class          no   \n",
       "1    Economy Class          no   \n",
       "2   Business Class         yes   \n",
       "3    Economy Class          no   \n",
       "4    Economy Class          no   \n",
       "5    Economy Class          no   \n",
       "6  Premium Economy         yes   \n",
       "7    Economy Class         yes   \n",
       "8    Economy Class         yes   \n",
       "9    Economy Class          no   \n",
       "\n",
       "                                           CleanText  \\\n",
       "0  hour takeoff received mail state cryptic messa...   \n",
       "1  recently delay bru lhr due staff shortage anno...   \n",
       "2  boarded time take age get runway due congestio...   \n",
       "3  day advise cancelled ask rebook hour hour orig...   \n",
       "4  travel lisbon dream vacation cruise portugal s...   \n",
       "5  book bucharest manchester layover heathrow del...   \n",
       "6  book online month ago hitch replacement meanin...   \n",
       "7  time crew polite story outward europe generall...   \n",
       "8  angry disappoint unsatisfied route london atla...   \n",
       "9  infrequent flyer always first choice reassuran...   \n",
       "\n",
       "                                              Tokens  Sentiment  \\\n",
       "0  ['hour', 'takeoff', 'received', 'mail', 'state...  -0.045588   \n",
       "1  ['recently', 'delay', 'bru', 'lhr', 'due', 'st...   0.006944   \n",
       "2  ['boarded', 'time', 'take', 'age', 'get', 'run...   0.128030   \n",
       "3  ['day', 'advise', 'cancelled', 'ask', 'rebook'...   0.113889   \n",
       "4  ['travel', 'lisbon', 'dream', 'vacation', 'cru...  -0.073958   \n",
       "5  ['book', 'bucharest', 'manchester', 'layover',...  -0.243958   \n",
       "6  ['book', 'online', 'month', 'ago', 'hitch', 'r...   0.080565   \n",
       "7  ['time', 'crew', 'polite', 'story', 'outward',...   0.212500   \n",
       "8  ['angry', 'disappoint', 'unsatisfied', 'route'...   0.024176   \n",
       "9  ['infrequent', 'flyer', 'always', 'first', 'ch...   0.087719   \n",
       "\n",
       "                                             Bigrams  \\\n",
       "0  [('hour', 'takeoff'), ('takeoff', 'received'),...   \n",
       "1  [('recently', 'delay'), ('delay', 'bru'), ('br...   \n",
       "2  [('boarded', 'time'), ('time', 'take'), ('take...   \n",
       "3  [('day', 'advise'), ('advise', 'cancelled'), (...   \n",
       "4  [('travel', 'lisbon'), ('lisbon', 'dream'), ('...   \n",
       "5  [('book', 'bucharest'), ('bucharest', 'manches...   \n",
       "6  [('book', 'online'), ('online', 'month'), ('mo...   \n",
       "7  [('time', 'crew'), ('crew', 'polite'), ('polit...   \n",
       "8  [('angry', 'disappoint'), ('disappoint', 'unsa...   \n",
       "9  [('infrequent', 'flyer'), ('flyer', 'always'),...   \n",
       "\n",
       "                                            Trigrams  \\\n",
       "0  [('hour', 'takeoff', 'received'), ('takeoff', ...   \n",
       "1  [('recently', 'delay', 'bru'), ('delay', 'bru'...   \n",
       "2  [('boarded', 'time', 'take'), ('time', 'take',...   \n",
       "3  [('day', 'advise', 'cancelled'), ('advise', 'c...   \n",
       "4  [('travel', 'lisbon', 'dream'), ('lisbon', 'dr...   \n",
       "5  [('book', 'bucharest', 'manchester'), ('buchar...   \n",
       "6  [('book', 'online', 'month'), ('online', 'mont...   \n",
       "7  [('time', 'crew', 'polite'), ('crew', 'polite'...   \n",
       "8  [('angry', 'disappoint', 'unsatisfied'), ('dis...   \n",
       "9  [('infrequent', 'flyer', 'always'), ('flyer', ...   \n",
       "\n",
       "                                          CleanText2  \n",
       "0  [hour, takeoff, received, mail, state, cryptic...  \n",
       "1  [recently, delay, bru, lhr, due, staff, shorta...  \n",
       "2  [boarded, time, take, age, get, runway, due, c...  \n",
       "3  [day, advise, cancelled, ask, rebook, hour, ho...  \n",
       "4  [travel, lisbon, dream, vacation, cruise, port...  \n",
       "5  [book, bucharest, manchester, layover, heathro...  \n",
       "6  [book, online, month, ago, hitch, replacement,...  \n",
       "7  [time, crew, polite, story, outward, europe, g...  \n",
       "8  [angry, disappoint, unsatisfied, route, london...  \n",
       "9  [infrequent, flyer, always, choice, reassuranc...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View cleaned DataFrame\n",
    "df_cleaned.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6106fb",
   "metadata": {},
   "source": [
    "# 2a) Splitting the dataset by SeatType\n",
    "\n",
    "Creating separate dataframes for each seat types for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc824f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_reset(df, seat_type):\n",
    "    return df[df['SeatType'] == seat_type].reset_index(drop=True)\n",
    "\n",
    "economy_df = filter_and_reset(df_cleaned, 'Economy Class')\n",
    "premium_economy_df = filter_and_reset(df_cleaned, 'Premium Economy')\n",
    "business_class_df = filter_and_reset(df_cleaned, 'Business Class')\n",
    "first_class_df = filter_and_reset(df_cleaned, 'First Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6c09f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        SeatType Recommended  \\\n",
      "0  Economy Class          no   \n",
      "1  Economy Class          no   \n",
      "2  Economy Class          no   \n",
      "3  Economy Class          no   \n",
      "4  Economy Class          no   \n",
      "\n",
      "                                           CleanText  \\\n",
      "0  hour takeoff received mail state cryptic messa...   \n",
      "1  recently delay bru lhr due staff shortage anno...   \n",
      "2  day advise cancelled ask rebook hour hour orig...   \n",
      "3  travel lisbon dream vacation cruise portugal s...   \n",
      "4  book bucharest manchester layover heathrow del...   \n",
      "\n",
      "                                              Tokens  Sentiment  \\\n",
      "0  ['hour', 'takeoff', 'received', 'mail', 'state...  -0.045588   \n",
      "1  ['recently', 'delay', 'bru', 'lhr', 'due', 'st...   0.006944   \n",
      "2  ['day', 'advise', 'cancelled', 'ask', 'rebook'...   0.113889   \n",
      "3  ['travel', 'lisbon', 'dream', 'vacation', 'cru...  -0.073958   \n",
      "4  ['book', 'bucharest', 'manchester', 'layover',...  -0.243958   \n",
      "\n",
      "                                             Bigrams  \\\n",
      "0  [('hour', 'takeoff'), ('takeoff', 'received'),...   \n",
      "1  [('recently', 'delay'), ('delay', 'bru'), ('br...   \n",
      "2  [('day', 'advise'), ('advise', 'cancelled'), (...   \n",
      "3  [('travel', 'lisbon'), ('lisbon', 'dream'), ('...   \n",
      "4  [('book', 'bucharest'), ('bucharest', 'manches...   \n",
      "\n",
      "                                            Trigrams  \\\n",
      "0  [('hour', 'takeoff', 'received'), ('takeoff', ...   \n",
      "1  [('recently', 'delay', 'bru'), ('delay', 'bru'...   \n",
      "2  [('day', 'advise', 'cancelled'), ('advise', 'c...   \n",
      "3  [('travel', 'lisbon', 'dream'), ('lisbon', 'dr...   \n",
      "4  [('book', 'bucharest', 'manchester'), ('buchar...   \n",
      "\n",
      "                                          CleanText2  \n",
      "0  [hour, takeoff, received, mail, state, cryptic...  \n",
      "1  [recently, delay, bru, lhr, due, staff, shorta...  \n",
      "2  [day, advise, cancelled, ask, rebook, hour, ho...  \n",
      "3  [travel, lisbon, dream, vacation, cruise, port...  \n",
      "4  [book, bucharest, manchester, layover, heathro...  \n",
      "          SeatType Recommended  \\\n",
      "0  Premium Economy         yes   \n",
      "1  Premium Economy          no   \n",
      "2  Premium Economy          no   \n",
      "3  Premium Economy          no   \n",
      "4  Premium Economy          no   \n",
      "\n",
      "                                           CleanText  \\\n",
      "0  book online month ago hitch replacement meanin...   \n",
      "1  initial cancel hour prior automatically resche...   \n",
      "2  care support shock write previously loyal trav...   \n",
      "3  express displeasure concern regard italy trip ...   \n",
      "4  family fly mostly last year priority passenger...   \n",
      "\n",
      "                                              Tokens  Sentiment  \\\n",
      "0  ['book', 'online', 'month', 'ago', 'hitch', 'r...   0.080565   \n",
      "1  ['initial', 'cancel', 'hour', 'prior', 'automa...  -0.053022   \n",
      "2  ['care', 'support', 'shock', 'write', 'previou...   0.066667   \n",
      "3  ['express', 'displeasure', 'concern', 'regard'...   0.105667   \n",
      "4  ['family', 'fly', 'mostly', 'last', 'year', 'p...   0.125269   \n",
      "\n",
      "                                             Bigrams  \\\n",
      "0  [('book', 'online'), ('online', 'month'), ('mo...   \n",
      "1  [('initial', 'cancel'), ('cancel', 'hour'), ('...   \n",
      "2  [('care', 'support'), ('support', 'shock'), ('...   \n",
      "3  [('express', 'displeasure'), ('displeasure', '...   \n",
      "4  [('family', 'fly'), ('fly', 'mostly'), ('mostl...   \n",
      "\n",
      "                                            Trigrams  \\\n",
      "0  [('book', 'online', 'month'), ('online', 'mont...   \n",
      "1  [('initial', 'cancel', 'hour'), ('cancel', 'ho...   \n",
      "2  [('care', 'support', 'shock'), ('support', 'sh...   \n",
      "3  [('express', 'displeasure', 'concern'), ('disp...   \n",
      "4  [('family', 'fly', 'mostly'), ('fly', 'mostly'...   \n",
      "\n",
      "                                          CleanText2  \n",
      "0  [book, online, month, ago, hitch, replacement,...  \n",
      "1  [initial, cancel, hour, prior, automatically, ...  \n",
      "2  [care, support, shock, write, previously, loya...  \n",
      "3  [express, displeasure, concern, regard, italy,...  \n",
      "4  [family, fly, mostly, last, year, priority, pa...  \n",
      "         SeatType Recommended  \\\n",
      "0  Business Class         yes   \n",
      "1  Business Class         yes   \n",
      "2  Business Class          no   \n",
      "3  Business Class          no   \n",
      "4  Business Class          no   \n",
      "\n",
      "                                           CleanText  \\\n",
      "0  boarded time take age get runway due congestio...   \n",
      "1  totally unremarkable time comfortable european...   \n",
      "2  flew istanbul london business class half child...   \n",
      "3  porto london heathrow operate finnair use finn...   \n",
      "4  frequent flyer last year note rating give fran...   \n",
      "\n",
      "                                              Tokens  Sentiment  \\\n",
      "0  ['boarded', 'time', 'take', 'age', 'get', 'run...   0.128030   \n",
      "1  ['totally', 'unremarkable', 'time', 'comfortab...  -0.073611   \n",
      "2  ['flew', 'istanbul', 'london', 'business', 'cl...  -0.009141   \n",
      "3  ['porto', 'london', 'heathrow', 'operate', 'fi...   0.110682   \n",
      "4  ['frequent', 'flyer', 'last', 'year', 'note', ...   0.168056   \n",
      "\n",
      "                                             Bigrams  \\\n",
      "0  [('boarded', 'time'), ('time', 'take'), ('take...   \n",
      "1  [('totally', 'unremarkable'), ('unremarkable',...   \n",
      "2  [('flew', 'istanbul'), ('istanbul', 'london'),...   \n",
      "3  [('porto', 'london'), ('london', 'heathrow'), ...   \n",
      "4  [('frequent', 'flyer'), ('flyer', 'last'), ('l...   \n",
      "\n",
      "                                            Trigrams  \\\n",
      "0  [('boarded', 'time', 'take'), ('time', 'take',...   \n",
      "1  [('totally', 'unremarkable', 'time'), ('unrema...   \n",
      "2  [('flew', 'istanbul', 'london'), ('istanbul', ...   \n",
      "3  [('porto', 'london', 'heathrow'), ('london', '...   \n",
      "4  [('frequent', 'flyer', 'last'), ('flyer', 'las...   \n",
      "\n",
      "                                          CleanText2  \n",
      "0  [boarded, time, take, age, get, runway, due, c...  \n",
      "1  [totally, unremarkable, time, comfortable, eur...  \n",
      "2  [flew, istanbul, london, half, child, watch, i...  \n",
      "3  [porto, london, heathrow, operate, finnair, us...  \n",
      "4  [frequent, flyer, last, year, note, rating, gi...  \n",
      "      SeatType Recommended                                          CleanText  \\\n",
      "0  First Class         yes  excellent service level proactive crew superb ...   \n",
      "1  First Class          no  airplane lounge wear old broken dallas heathro...   \n",
      "2  First Class          no  late affront stand first class line hour final...   \n",
      "3  First Class         yes  excellent service ground board first class pro...   \n",
      "4  First Class          no  shamble start well excellent check first mid a...   \n",
      "\n",
      "                                              Tokens  Sentiment  \\\n",
      "0  ['excellent', 'service', 'level', 'proactive',...   0.349883   \n",
      "1  ['airplane', 'lounge', 'wear', 'old', 'broken'...  -0.067500   \n",
      "2  ['late', 'affront', 'stand', 'first', 'class',...   0.182857   \n",
      "3  ['excellent', 'service', 'ground', 'board', 'f...   0.311905   \n",
      "4  ['shamble', 'start', 'well', 'excellent', 'che...   0.186957   \n",
      "\n",
      "                                             Bigrams  \\\n",
      "0  [('excellent', 'service'), ('service', 'level'...   \n",
      "1  [('airplane', 'lounge'), ('lounge', 'wear'), (...   \n",
      "2  [('late', 'affront'), ('affront', 'stand'), ('...   \n",
      "3  [('excellent', 'service'), ('service', 'ground...   \n",
      "4  [('shamble', 'start'), ('start', 'well'), ('we...   \n",
      "\n",
      "                                            Trigrams  \\\n",
      "0  [('excellent', 'service', 'level'), ('service'...   \n",
      "1  [('airplane', 'lounge', 'wear'), ('lounge', 'w...   \n",
      "2  [('late', 'affront', 'stand'), ('affront', 'st...   \n",
      "3  [('excellent', 'service', 'ground'), ('service...   \n",
      "4  [('shamble', 'start', 'well'), ('start', 'well...   \n",
      "\n",
      "                                          CleanText2  \n",
      "0  [excellent, service, level, proactive, crew, s...  \n",
      "1  [airplane, lounge, wear, old, broken, dallas, ...  \n",
      "2  [late, affront, stand, line, hour, finally, fr...  \n",
      "3  [excellent, service, ground, board, product, m...  \n",
      "4  [shamble, start, well, excellent, check, mid, ...  \n"
     ]
    }
   ],
   "source": [
    "# test print \n",
    "print(economy_df.head())\n",
    "print(premium_economy_df.head())\n",
    "print(business_class_df.head())\n",
    "print(first_class_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81f3eca",
   "metadata": {},
   "source": [
    "# 2b) Creating TF-IDF vectorizer for each SeatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88d382e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating TF-IDF features for Economy Class\n",
      "    airport    anyhow     avoid       bad  capacity     check   cryptic  \\\n",
      "0  0.067140  0.241413  0.103781  0.065405  0.191424  0.104833  0.278865   \n",
      "1  0.000000  0.000000  0.000000  0.051152  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  0.000000  0.050328  0.000000  0.161334  0.000000   \n",
      "3  0.109771  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "4  0.118266  0.000000  0.000000  0.057605  0.000000  0.000000  0.000000   \n",
      "\n",
      "      delay  departure  disruption  ...  lhrjfklaxlhr  presuming  polystyrene  \\\n",
      "0  0.060731   0.084876    0.231605  ...           0.0        0.0          0.0   \n",
      "1  0.189988   0.000000    0.000000  ...           0.0        0.0          0.0   \n",
      "2  0.000000   0.000000    0.000000  ...           0.0        0.0          0.0   \n",
      "3  0.000000   0.000000    0.000000  ...           0.0        0.0          0.0   \n",
      "4  0.106976   0.000000    0.000000  ...           0.0        0.0          0.0   \n",
      "\n",
      "   installed  grimace  citynew  inconveniently  quintessentially  sellby  \\\n",
      "0        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "1        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "2        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "3        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "4        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "\n",
      "   sinlhr  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "\n",
      "[5 rows x 11306 columns]\n",
      "Generating TF-IDF features for Premium Economy Class\n",
      "    airport  anyhow     avoid       bad  capacity     check  cryptic  \\\n",
      "0  0.000000     0.0  0.000000  0.000000       0.0  0.000000      0.0   \n",
      "1  0.000000     0.0  0.093697  0.000000       0.0  0.000000      0.0   \n",
      "2  0.079437     0.0  0.000000  0.000000       0.0  0.062017      0.0   \n",
      "3  0.000000     0.0  0.000000  0.000000       0.0  0.024255      0.0   \n",
      "4  0.000000     0.0  0.000000  0.043331       0.0  0.069453      0.0   \n",
      "\n",
      "      delay  departure  disruption  ...  lhrjfklaxlhr  presuming  polystyrene  \\\n",
      "0  0.051857   0.072474         0.0  ...           0.0        0.0          0.0   \n",
      "1  0.054830   0.000000         0.0  ...           0.0        0.0          0.0   \n",
      "2  0.000000   0.000000         0.0  ...           0.0        0.0          0.0   \n",
      "3  0.028102   0.039275         0.0  ...           0.0        0.0          0.0   \n",
      "4  0.000000   0.000000         0.0  ...           0.0        0.0          0.0   \n",
      "\n",
      "   installed  grimace  citynew  inconveniently  quintessentially  sellby  \\\n",
      "0        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "1        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "2        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "3        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "4        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "\n",
      "   sinlhr  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "\n",
      "[5 rows x 11306 columns]\n",
      "Generating TF-IDF features for Business Class\n",
      "   airport  anyhow  avoid       bad  capacity  check  cryptic     delay  \\\n",
      "0      0.0     0.0    0.0  0.000000  0.000000    0.0      0.0  0.000000   \n",
      "1      0.0     0.0    0.0  0.000000  0.000000    0.0      0.0  0.000000   \n",
      "2      0.0     0.0    0.0  0.138867  0.000000    0.0      0.0  0.000000   \n",
      "3      0.0     0.0    0.0  0.000000  0.000000    0.0      0.0  0.049525   \n",
      "4      0.0     0.0    0.0  0.000000  0.098641    0.0      0.0  0.000000   \n",
      "\n",
      "   departure  disruption  ...  lhrjfklaxlhr  presuming  polystyrene  \\\n",
      "0        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "1        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "2        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "3        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "4        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "\n",
      "   installed  grimace  citynew  inconveniently  quintessentially  sellby  \\\n",
      "0        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "1        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "2        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "3        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "4        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "\n",
      "   sinlhr  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "\n",
      "[5 rows x 11306 columns]\n",
      "Generating TF-IDF features for First Class\n",
      "   airport  anyhow     avoid  bad  capacity     check  cryptic     delay  \\\n",
      "0      0.0     0.0  0.000000  0.0       0.0  0.000000      0.0  0.000000   \n",
      "1      0.0     0.0  0.000000  0.0       0.0  0.000000      0.0  0.000000   \n",
      "2      0.0     0.0  0.000000  0.0       0.0  0.000000      0.0  0.000000   \n",
      "3      0.0     0.0  0.000000  0.0       0.0  0.000000      0.0  0.112224   \n",
      "4      0.0     0.0  0.058702  0.0       0.0  0.029649      0.0  0.000000   \n",
      "\n",
      "   departure  disruption  ...  lhrjfklaxlhr  presuming  polystyrene  \\\n",
      "0        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "1        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "2        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "3        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "4        0.0         0.0  ...           0.0        0.0          0.0   \n",
      "\n",
      "   installed  grimace  citynew  inconveniently  quintessentially  sellby  \\\n",
      "0        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "1        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "2        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "3        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "4        0.0      0.0      0.0             0.0               0.0     0.0   \n",
      "\n",
      "   sinlhr  \n",
      "0     0.0  \n",
      "1     0.0  \n",
      "2     0.0  \n",
      "3     0.0  \n",
      "4     0.0  \n",
      "\n",
      "[5 rows x 11306 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Build the dictionary (CleanText2 for tokenised)\n",
    "mydict = corpora.Dictionary(df_cleaned['CleanText2'])\n",
    "vocab_len = len(mydict)\n",
    "corpus = [mydict.doc2bow(tokens) for tokens in df_cleaned['CleanText2']]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "def get_tfidf_features(df_cleaned, column_name='CleanText2'):\n",
    "    def process_row(tokens):\n",
    "        doc = mydict.doc2bow(tokens)\n",
    "        # Converting the tokens into the format that the model requires\n",
    "        features = gensim.matutils.corpus2csc([tfidf_model[doc]], num_terms=vocab_len).toarray()[:,0]\n",
    "        return features\n",
    "    \n",
    "    # Generate TF-IDF features\n",
    "    return pd.DataFrame([process_row(tokens) for tokens in df_cleaned[column_name]], \n",
    "                        index=df_cleaned.index,\n",
    "                        columns=[mydict[i] for i in range(vocab_len)])\n",
    "\n",
    "# Applying TF-IDF to each seat type\n",
    "def apply_tfidf_to_all_seat_types():\n",
    "    tfidf_dfs = {}\n",
    "    \n",
    "    seat_types = {\n",
    "        'Economy Class': economy_df,\n",
    "        'Premium Economy Class': premium_economy_df,\n",
    "        'Business Class': business_class_df,\n",
    "        'First Class': first_class_df\n",
    "    }\n",
    "    \n",
    "    for seat_type, df in seat_types.items():\n",
    "        print(f\"Generating TF-IDF features for {seat_type}\")\n",
    "        tfidf_df = get_tfidf_features(df, 'CleanText2')\n",
    "        tfidf_dfs[seat_type] = tfidf_df\n",
    "        print(tfidf_df.head())\n",
    "    return tfidf_dfs\n",
    "\n",
    "# Generate TF-IDFs for all seat types\n",
    "tfidf_features_all = apply_tfidf_to_all_seat_types()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ffb568",
   "metadata": {},
   "source": [
    "# 3) Creating the LDA Model for topic analysis and analysing alongside bigrams and trigrams\n",
    "\n",
    "LDA Model is created to identify the common topics mentioned by customers for each seat type. With the top topics generated for each seat type, top bigrams/trigrams will then be analysed alongside as well. Thereafter, bigrams/trigrams generated containing the most common topcis will be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75b2abc",
   "metadata": {},
   "source": [
    "# 3a) Creating LDA modelling for topic analysis of each Seat Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f79827",
   "metadata": {},
   "source": [
    "Creating the LDA model for the \"Economy Class\" to identify top topics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90ad09b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.028*\"seat\" + 0.012*\"cabin\" + 0.011*\"heathrow\" + 0.010*\"london\" + '\n",
      "  '0.010*\"crew\" + 0.009*\"fly\" + 0.009*\"service\" + 0.009*\"time\" + 0.009*\"meal\" '\n",
      "  '+ 0.008*\"food\"'),\n",
      " (1,\n",
      "  '0.012*\"heathrow\" + 0.011*\"fly\" + 0.011*\"hour\" + 0.011*\"seat\" + '\n",
      "  '0.011*\"service\" + 0.010*\"staff\" + 0.010*\"make\" + 0.009*\"would\" + '\n",
      "  '0.009*\"travel\" + 0.008*\"london\"'),\n",
      " (2,\n",
      "  '0.018*\"seat\" + 0.011*\"cabin\" + 0.011*\"fly\" + 0.011*\"service\" + 0.010*\"time\" '\n",
      "  '+ 0.010*\"good\" + 0.010*\"food\" + 0.009*\"crew\" + 0.008*\"london\" + '\n",
      "  '0.008*\"well\"'),\n",
      " (3,\n",
      "  '0.020*\"get\" + 0.018*\"hour\" + 0.014*\"customer\" + 0.013*\"tell\" + 0.012*\"day\" '\n",
      "  '+ 0.012*\"would\" + 0.010*\"london\" + 0.010*\"service\" + 0.009*\"book\" + '\n",
      "  '0.009*\"back\"'),\n",
      " (4,\n",
      "  '0.033*\"seat\" + 0.020*\"good\" + 0.015*\"crew\" + 0.014*\"cabin\" + 0.014*\"food\" + '\n",
      "  '0.013*\"service\" + 0.011*\"london\" + 0.010*\"fly\" + 0.009*\"entertainment\" + '\n",
      "  '0.008*\"great\"'),\n",
      " (5,\n",
      "  '0.007*\"seat\" + 0.007*\"hour\" + 0.006*\"time\" + 0.006*\"food\" + 0.005*\"london\" '\n",
      "  '+ 0.005*\"cost\" + 0.005*\"leg\" + 0.005*\"low\" + 0.005*\"heathrow\" + '\n",
      "  '0.005*\"escalator\"'),\n",
      " (6,\n",
      "  '0.031*\"seat\" + 0.011*\"pay\" + 0.011*\"london\" + 0.010*\"get\" + 0.009*\"time\" + '\n",
      "  '0.009*\"would\" + 0.009*\"book\" + 0.009*\"food\" + 0.008*\"extra\" + 0.007*\"fly\"'),\n",
      " (7,\n",
      "  '0.013*\"seat\" + 0.011*\"senior\" + 0.010*\"one\" + 0.009*\"staff\" + 0.009*\"ask\" + '\n",
      "  '0.008*\"get\" + 0.008*\"go\" + 0.007*\"assistance\" + 0.006*\"child\" + '\n",
      "  '0.005*\"infant\"'),\n",
      " (8,\n",
      "  '0.016*\"seat\" + 0.010*\"drink\" + 0.008*\"crew\" + 0.008*\"staff\" + 0.008*\"food\" '\n",
      "  '+ 0.008*\"get\" + 0.008*\"board\" + 0.008*\"meal\" + 0.006*\"passenger\" + '\n",
      "  '0.006*\"cabin\"'),\n",
      " (9,\n",
      "  '0.010*\"food\" + 0.009*\"seat\" + 0.009*\"london\" + 0.009*\"get\" + '\n",
      "  '0.008*\"service\" + 0.008*\"even\" + 0.008*\"offer\" + 0.008*\"fly\" + 0.007*\"meal\" '\n",
      "  '+ 0.007*\"time\"'),\n",
      " (10,\n",
      "  '0.022*\"food\" + 0.019*\"crew\" + 0.013*\"drink\" + 0.012*\"seat\" + '\n",
      "  '0.011*\"service\" + 0.010*\"london\" + 0.010*\"time\" + 0.009*\"screen\" + '\n",
      "  '0.009*\"cabin\" + 0.008*\"back\"'),\n",
      " (11,\n",
      "  '0.014*\"good\" + 0.012*\"seat\" + 0.011*\"board\" + 0.010*\"delay\" + 0.010*\"time\" '\n",
      "  '+ 0.010*\"hour\" + 0.010*\"get\" + 0.010*\"staff\" + 0.009*\"heathrow\" + '\n",
      "  '0.009*\"late\"'),\n",
      " (12,\n",
      "  '0.009*\"london\" + 0.009*\"back\" + 0.008*\"fly\" + 0.008*\"never\" + 0.007*\"get\" + '\n",
      "  '0.007*\"use\" + 0.006*\"staff\" + 0.006*\"luggage\" + 0.006*\"still\" + '\n",
      "  '0.006*\"ticket\"'),\n",
      " (13,\n",
      "  '0.044*\"seat\" + 0.012*\"crew\" + 0.010*\"time\" + 0.010*\"get\" + 0.010*\"cabin\" + '\n",
      "  '0.010*\"hour\" + 0.009*\"fly\" + 0.008*\"service\" + 0.008*\"front\" + '\n",
      "  '0.007*\"would\"'),\n",
      " (14,\n",
      "  '0.020*\"seat\" + 0.018*\"service\" + 0.016*\"crew\" + 0.014*\"time\" + 0.014*\"good\" '\n",
      "  '+ 0.012*\"board\" + 0.012*\"drink\" + 0.010*\"heathrow\" + 0.009*\"full\" + '\n",
      "  '0.009*\"cabin\"'),\n",
      " (15,\n",
      "  '0.013*\"meal\" + 0.010*\"offer\" + 0.009*\"get\" + 0.007*\"water\" + 0.006*\"pay\" + '\n",
      "  '0.006*\"one\" + 0.006*\"return\" + 0.006*\"come\" + 0.006*\"say\" + 0.005*\"london\"'),\n",
      " (16,\n",
      "  '0.014*\"check\" + 0.012*\"service\" + 0.011*\"pay\" + 0.010*\"bag\" + 0.010*\"cabin\" '\n",
      "  '+ 0.009*\"food\" + 0.008*\"seat\" + 0.008*\"staff\" + 0.008*\"crew\" + '\n",
      "  '0.008*\"time\"'),\n",
      " (17,\n",
      "  '0.017*\"crew\" + 0.017*\"delay\" + 0.015*\"time\" + 0.014*\"service\" + '\n",
      "  '0.014*\"board\" + 0.012*\"good\" + 0.010*\"food\" + 0.010*\"staff\" + 0.010*\"cabin\" '\n",
      "  '+ 0.009*\"make\"'),\n",
      " (18,\n",
      "  '0.016*\"call\" + 0.015*\"airport\" + 0.015*\"get\" + 0.015*\"luggage\" + '\n",
      "  '0.013*\"time\" + 0.012*\"check\" + 0.010*\"tell\" + 0.010*\"hour\" + 0.010*\"london\" '\n",
      "  '+ 0.009*\"take\"'),\n",
      " (19,\n",
      "  '0.018*\"time\" + 0.014*\"service\" + 0.011*\"customer\" + 0.011*\"try\" + '\n",
      "  '0.010*\"call\" + 0.008*\"even\" + 0.007*\"fly\" + 0.007*\"lounge\" + 0.007*\"get\" + '\n",
      "  '0.006*\"last\"')]\n"
     ]
    }
   ],
   "source": [
    "# LDA Model for Economy Class\n",
    "topic_num = 20\n",
    "word_num = 10\n",
    "\n",
    "# Use the tokenized reviews in the CleanText2 column\n",
    "tokenized_reviews_EC = economy_df['CleanText2']\n",
    "\n",
    "# Creating dictionary and document-term matrix\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_reviews_EC)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_reviews_EC]\n",
    "\n",
    "# Creating the LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=topic_num, id2word=dictionary, passes=20)\n",
    "\n",
    "# Printing the topics\n",
    "pprint(ldamodel.print_topics(num_topics=topic_num, num_words=word_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd95fcd",
   "metadata": {},
   "source": [
    "# Computing perplexity and coherence scores to pick best number of topics\n",
    "\n",
    "For this portion, I had tested for topic numbers between 5 to 30, and it was found that for topic number 20, it had the lowest scores for perplexity and highest scores for coherence. Hence, topic_num = 20 was chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d6501e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -7.5330971965750235\n"
     ]
    }
   ],
   "source": [
    "#compute perplexity \n",
    "print('Perplexity: ', ldamodel.log_perplexity(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "629cd2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5135620403198233"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#compute coherence \n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "Coherence_Model = CoherenceModel(model=ldamodel, corpus=doc_term_matrix,coherence='u_mass')\n",
    "Coherence_Model.get_coherence()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41a0c4",
   "metadata": {},
   "source": [
    "Creating the LDA model for the \"Premium Economy\" class to identify top topics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2678288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"seat\" + 0.017*\"service\" + 0.014*\"staff\" + 0.012*\"cabin\" + '\n",
      "  '0.011*\"traveller\" + 0.009*\"would\" + 0.009*\"world\" + 0.009*\"food\" + '\n",
      "  '0.008*\"fly\" + 0.007*\"time\"'),\n",
      " (1,\n",
      "  '0.019*\"seat\" + 0.013*\"would\" + 0.012*\"food\" + 0.012*\"meal\" + '\n",
      "  '0.009*\"experience\" + 0.008*\"good\" + 0.008*\"crew\" + 0.007*\"product\" + '\n",
      "  '0.007*\"window\" + 0.007*\"get\"'),\n",
      " (2,\n",
      "  '0.016*\"crew\" + 0.014*\"good\" + 0.012*\"food\" + 0.012*\"cabin\" + '\n",
      "  '0.012*\"service\" + 0.011*\"drink\" + 0.011*\"seat\" + 0.010*\"offer\" + '\n",
      "  '0.009*\"meal\" + 0.009*\"one\"'),\n",
      " (3,\n",
      "  '0.016*\"seat\" + 0.012*\"get\" + 0.011*\"singapore\" + 0.011*\"service\" + '\n",
      "  '0.011*\"bottle\" + 0.008*\"staff\" + 0.008*\"baby\" + 0.008*\"london\" + '\n",
      "  '0.008*\"time\" + 0.008*\"year\"'),\n",
      " (4,\n",
      "  '0.030*\"seat\" + 0.013*\"cabin\" + 0.012*\"world\" + 0.011*\"good\" + '\n",
      "  '0.010*\"traveller\" + 0.010*\"get\" + 0.010*\"leg\" + 0.010*\"long\" + 0.008*\"meal\" '\n",
      "  '+ 0.008*\"much\"'),\n",
      " (5,\n",
      "  '0.013*\"get\" + 0.013*\"london\" + 0.012*\"fly\" + 0.011*\"seat\" + 0.011*\"service\" '\n",
      "  '+ 0.010*\"book\" + 0.007*\"pay\" + 0.007*\"even\" + 0.007*\"would\" + 0.006*\"one\"'),\n",
      " (6,\n",
      "  '0.016*\"good\" + 0.014*\"seat\" + 0.013*\"food\" + 0.011*\"check\" + 0.011*\"staff\" '\n",
      "  '+ 0.010*\"return\" + 0.009*\"fly\" + 0.008*\"service\" + 0.008*\"world\" + '\n",
      "  '0.008*\"crew\"'),\n",
      " (7,\n",
      "  '0.026*\"service\" + 0.012*\"pay\" + 0.012*\"customer\" + 0.011*\"seat\" + '\n",
      "  '0.010*\"upgrade\" + 0.009*\"get\" + 0.009*\"hour\" + 0.008*\"would\" + '\n",
      "  '0.008*\"staff\" + 0.007*\"cabin\"'),\n",
      " (8,\n",
      "  '0.026*\"seat\" + 0.011*\"good\" + 0.010*\"back\" + 0.010*\"hour\" + 0.009*\"get\" + '\n",
      "  '0.009*\"food\" + 0.008*\"service\" + 0.007*\"would\" + 0.006*\"london\" + '\n",
      "  '0.006*\"book\"'),\n",
      " (9,\n",
      "  '0.013*\"fly\" + 0.010*\"crew\" + 0.010*\"service\" + 0.009*\"good\" + 0.009*\"food\" '\n",
      "  '+ 0.008*\"cabin\" + 0.008*\"london\" + 0.008*\"well\" + 0.008*\"back\" + '\n",
      "  '0.008*\"board\"'),\n",
      " (10,\n",
      "  '0.019*\"staff\" + 0.012*\"service\" + 0.012*\"get\" + 0.010*\"good\" + 0.009*\"leg\" '\n",
      "  '+ 0.009*\"time\" + 0.008*\"seat\" + 0.008*\"bad\" + 0.008*\"hour\" + 0.007*\"make\"'),\n",
      " (11,\n",
      "  '0.011*\"heathrow\" + 0.011*\"would\" + 0.010*\"delay\" + 0.010*\"attendant\" + '\n",
      "  '0.010*\"time\" + 0.009*\"hour\" + 0.009*\"tea\" + 0.009*\"service\" + 0.008*\"ask\" + '\n",
      "  '0.008*\"departure\"'),\n",
      " (12,\n",
      "  '0.016*\"london\" + 0.014*\"seat\" + 0.010*\"food\" + 0.010*\"fly\" + 0.009*\"time\" + '\n",
      "  '0.009*\"hour\" + 0.008*\"return\" + 0.008*\"get\" + 0.008*\"one\" + 0.008*\"year\"'),\n",
      " (13,\n",
      "  '0.014*\"seat\" + 0.012*\"give\" + 0.010*\"good\" + 0.010*\"change\" + 0.010*\"world\" '\n",
      "  '+ 0.009*\"extra\" + 0.009*\"staff\" + 0.009*\"make\" + 0.008*\"book\" + '\n",
      "  '0.008*\"dinner\"'),\n",
      " (14,\n",
      "  '0.020*\"seat\" + 0.018*\"service\" + 0.012*\"staff\" + 0.009*\"extra\" + '\n",
      "  '0.009*\"london\" + 0.008*\"good\" + 0.008*\"cabin\" + 0.008*\"fly\" + 0.007*\"food\" '\n",
      "  '+ 0.007*\"get\"'),\n",
      " (15,\n",
      "  '0.011*\"make\" + 0.010*\"seat\" + 0.010*\"good\" + 0.009*\"time\" + 0.008*\"service\" '\n",
      "  '+ 0.008*\"food\" + 0.008*\"london\" + 0.007*\"delay\" + 0.007*\"board\" + '\n",
      "  '0.006*\"old\"'),\n",
      " (16,\n",
      "  '0.016*\"seat\" + 0.013*\"service\" + 0.011*\"food\" + 0.010*\"virgin\" + '\n",
      "  '0.009*\"time\" + 0.008*\"board\" + 0.008*\"juice\" + 0.008*\"small\" + 0.007*\"meal\" '\n",
      "  '+ 0.006*\"toilet\"'),\n",
      " (17,\n",
      "  '0.015*\"good\" + 0.014*\"service\" + 0.012*\"world\" + 0.012*\"seat\" + '\n",
      "  '0.010*\"board\" + 0.009*\"london\" + 0.008*\"cabin\" + 0.007*\"return\" + '\n",
      "  '0.007*\"drink\" + 0.007*\"time\"'),\n",
      " (18,\n",
      "  '0.024*\"seat\" + 0.020*\"get\" + 0.018*\"call\" + 0.010*\"try\" + 0.009*\"customer\" '\n",
      "  '+ 0.008*\"pay\" + 0.008*\"food\" + 0.007*\"tell\" + 0.007*\"book\" + 0.007*\"fly\"'),\n",
      " (19,\n",
      "  '0.020*\"seat\" + 0.015*\"fly\" + 0.010*\"food\" + 0.009*\"good\" + 0.009*\"service\" '\n",
      "  '+ 0.009*\"cabin\" + 0.009*\"hour\" + 0.008*\"get\" + 0.008*\"crew\" + 0.008*\"year\"')]\n"
     ]
    }
   ],
   "source": [
    "# LDA Model for Premium Economy Class\n",
    "topic_num = 20\n",
    "word_num = 10\n",
    "\n",
    "# Use the tokenized reviews in the CleanText2 column\n",
    "tokenized_reviews_PE = premium_economy_df['CleanText2']\n",
    "\n",
    "# Creating dictionary and document-term matrix\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_reviews_PE)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_reviews_PE]\n",
    "\n",
    "# Creating the LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=topic_num, id2word=dictionary, passes=20)\n",
    "\n",
    "# Printing the topics\n",
    "pprint(ldamodel.print_topics(num_topics=topic_num, num_words=word_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca05a46b",
   "metadata": {},
   "source": [
    "Creating the LDA model for the \"Business Class\" to identify top topics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2524f215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.013*\"seat\" + 0.010*\"fly\" + 0.010*\"offer\" + 0.009*\"food\" + 0.009*\"club\" + '\n",
      "  '0.009*\"staff\" + 0.008*\"new\" + 0.008*\"europe\" + 0.007*\"hour\" + '\n",
      "  '0.007*\"experience\"'),\n",
      " (1,\n",
      "  '0.014*\"seat\" + 0.011*\"new\" + 0.011*\"london\" + 0.010*\"even\" + 0.010*\"crew\" + '\n",
      "  '0.010*\"food\" + 0.010*\"service\" + 0.009*\"cabin\" + 0.009*\"good\" + '\n",
      "  '0.008*\"fly\"'),\n",
      " (2,\n",
      "  '0.012*\"crew\" + 0.011*\"experience\" + 0.010*\"one\" + 0.008*\"club\" + '\n",
      "  '0.008*\"food\" + 0.007*\"new\" + 0.006*\"world\" + 0.006*\"lounge\" + 0.006*\"hour\" '\n",
      "  '+ 0.006*\"time\"'),\n",
      " (3,\n",
      "  '0.029*\"seat\" + 0.012*\"get\" + 0.008*\"crew\" + 0.006*\"passenger\" + '\n",
      "  '0.006*\"time\" + 0.006*\"ask\" + 0.006*\"cabin\" + 0.005*\"say\" + 0.005*\"club\" + '\n",
      "  '0.005*\"row\"'),\n",
      " (4,\n",
      "  '0.010*\"lounge\" + 0.009*\"crew\" + 0.009*\"service\" + 0.008*\"good\" + '\n",
      "  '0.008*\"club\" + 0.008*\"would\" + 0.007*\"delay\" + 0.006*\"late\" + 0.006*\"get\" + '\n",
      "  '0.006*\"meal\"'),\n",
      " (5,\n",
      "  '0.036*\"seat\" + 0.008*\"passenger\" + 0.007*\"get\" + 0.006*\"crew\" + '\n",
      "  '0.006*\"attendant\" + 0.005*\"take\" + 0.005*\"aisle\" + 0.005*\"screen\" + '\n",
      "  '0.005*\"cabin\" + 0.005*\"food\"'),\n",
      " (6,\n",
      "  '0.020*\"seat\" + 0.013*\"food\" + 0.012*\"lounge\" + 0.008*\"fly\" + 0.008*\"london\" '\n",
      "  '+ 0.008*\"average\" + 0.006*\"return\" + 0.006*\"year\" + 0.005*\"air\" + '\n",
      "  '0.005*\"bad\"'),\n",
      " (7,\n",
      "  '0.020*\"service\" + 0.010*\"customer\" + 0.009*\"london\" + 0.009*\"seat\" + '\n",
      "  '0.008*\"crew\" + 0.007*\"pay\" + 0.007*\"return\" + 0.006*\"good\" + 0.006*\"offer\" '\n",
      "  '+ 0.006*\"experience\"'),\n",
      " (8,\n",
      "  '0.039*\"seat\" + 0.016*\"service\" + 0.013*\"get\" + 0.012*\"pay\" + 0.011*\"london\" '\n",
      "  '+ 0.009*\"fly\" + 0.008*\"food\" + 0.008*\"would\" + 0.007*\"return\" + '\n",
      "  '0.007*\"one\"'),\n",
      " (9,\n",
      "  '0.019*\"seat\" + 0.011*\"food\" + 0.008*\"cabin\" + 0.007*\"heathrow\" + '\n",
      "  '0.007*\"meal\" + 0.007*\"money\" + 0.006*\"london\" + 0.006*\"standard\" + '\n",
      "  '0.006*\"cramp\" + 0.006*\"waste\"'),\n",
      " (10,\n",
      "  '0.042*\"seat\" + 0.020*\"club\" + 0.019*\"good\" + 0.014*\"service\" + 0.014*\"food\" '\n",
      "  '+ 0.013*\"cabin\" + 0.011*\"crew\" + 0.009*\"world\" + 0.009*\"europe\" + '\n",
      "  '0.008*\"return\"'),\n",
      " (11,\n",
      "  '0.024*\"good\" + 0.018*\"lounge\" + 0.017*\"crew\" + 0.015*\"food\" + 0.014*\"time\" '\n",
      "  '+ 0.013*\"service\" + 0.012*\"seat\" + 0.012*\"board\" + 0.012*\"club\" + '\n",
      "  '0.012*\"cabin\"'),\n",
      " (12,\n",
      "  '0.030*\"seat\" + 0.011*\"get\" + 0.011*\"cabin\" + 0.010*\"club\" + 0.010*\"one\" + '\n",
      "  '0.009*\"fly\" + 0.008*\"would\" + 0.007*\"service\" + 0.006*\"crew\" + '\n",
      "  '0.005*\"come\"'),\n",
      " (13,\n",
      "  '0.015*\"time\" + 0.013*\"crew\" + 0.012*\"service\" + 0.009*\"check\" + '\n",
      "  '0.008*\"board\" + 0.008*\"staff\" + 0.007*\"one\" + 0.006*\"hour\" + 0.006*\"cabin\" '\n",
      "  '+ 0.006*\"serve\"'),\n",
      " (14,\n",
      "  '0.020*\"seat\" + 0.016*\"club\" + 0.012*\"food\" + 0.011*\"cabin\" + 0.011*\"world\" '\n",
      "  '+ 0.010*\"time\" + 0.009*\"service\" + 0.008*\"get\" + 0.008*\"good\" + '\n",
      "  '0.008*\"crew\"'),\n",
      " (15,\n",
      "  '0.014*\"work\" + 0.010*\"london\" + 0.007*\"food\" + 0.007*\"husband\" + '\n",
      "  '0.006*\"boeing\" + 0.006*\"fly\" + 0.005*\"old\" + 0.005*\"munich\" + '\n",
      "  '0.005*\"boston\" + 0.005*\"mumbai\"'),\n",
      " (16,\n",
      "  '0.017*\"one\" + 0.012*\"seat\" + 0.011*\"ask\" + 0.010*\"give\" + 0.010*\"club\" + '\n",
      "  '0.008*\"tell\" + 0.007*\"service\" + 0.007*\"meal\" + 0.006*\"offer\" + '\n",
      "  '0.006*\"day\"'),\n",
      " (17,\n",
      "  '0.013*\"get\" + 0.013*\"service\" + 0.012*\"passenger\" + 0.012*\"fly\" + '\n",
      "  '0.010*\"hour\" + 0.009*\"customer\" + 0.008*\"could\" + 0.008*\"time\" + '\n",
      "  '0.008*\"club\" + 0.008*\"year\"'),\n",
      " (18,\n",
      "  '0.026*\"seat\" + 0.009*\"service\" + 0.008*\"time\" + 0.008*\"lounge\" + '\n",
      "  '0.008*\"food\" + 0.008*\"staff\" + 0.007*\"one\" + 0.007*\"poor\" + 0.007*\"cabin\" + '\n",
      "  '0.006*\"passenger\"'),\n",
      " (19,\n",
      "  '0.013*\"seat\" + 0.012*\"hour\" + 0.010*\"time\" + 0.010*\"fly\" + 0.009*\"london\" + '\n",
      "  '0.008*\"delay\" + 0.008*\"heathrow\" + 0.007*\"take\" + 0.007*\"get\" + '\n",
      "  '0.007*\"luggage\"')]\n"
     ]
    }
   ],
   "source": [
    "# LDA Model for Business Class\n",
    "topic_num = 20\n",
    "word_num = 10\n",
    "\n",
    "# Use the tokenized reviews in the CleanText2 column\n",
    "tokenized_reviews_BC = business_class_df['CleanText2']\n",
    "\n",
    "# Creating dictionary and document-term matrix\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_reviews_BC)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_reviews_BC]\n",
    "\n",
    "# Creating the LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=topic_num, id2word=dictionary, passes=20)\n",
    "\n",
    "# Printing the topics\n",
    "pprint(ldamodel.print_topics(num_topics=topic_num, num_words=word_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641027df",
   "metadata": {},
   "source": [
    "Creating the LDA model for the \"First Class\" to identify top topics of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be5847d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"great\" + 0.013*\"crew\" + 0.011*\"food\" + 0.011*\"staff\" + 0.010*\"good\" '\n",
      "  '+ 0.010*\"service\" + 0.009*\"lounge\" + 0.008*\"return\" + 0.008*\"would\" + '\n",
      "  '0.008*\"passenger\"'),\n",
      " (1,\n",
      "  '0.019*\"crew\" + 0.013*\"good\" + 0.011*\"seat\" + 0.010*\"london\" + 0.008*\"cabin\" '\n",
      "  '+ 0.008*\"get\" + 0.008*\"day\" + 0.007*\"excellent\" + 0.007*\"two\" + '\n",
      "  '0.006*\"make\"'),\n",
      " (2,\n",
      "  '0.022*\"seat\" + 0.012*\"good\" + 0.012*\"crew\" + 0.012*\"service\" + '\n",
      "  '0.011*\"lounge\" + 0.011*\"well\" + 0.010*\"food\" + 0.010*\"experience\" + '\n",
      "  '0.008*\"heathrow\" + 0.008*\"club\"'),\n",
      " (3,\n",
      "  '0.011*\"good\" + 0.011*\"day\" + 0.011*\"case\" + 0.008*\"make\" + 0.008*\"usd\" + '\n",
      "  '0.007*\"give\" + 0.007*\"show\" + 0.007*\"club\" + 0.006*\"dinner\" + '\n",
      "  '0.006*\"selection\"'),\n",
      " (4,\n",
      "  '0.019*\"good\" + 0.015*\"food\" + 0.013*\"seat\" + 0.010*\"london\" + '\n",
      "  '0.010*\"heathrow\" + 0.010*\"return\" + 0.010*\"new\" + 0.008*\"club\" + '\n",
      "  '0.008*\"service\" + 0.007*\"crew\"'),\n",
      " (5,\n",
      "  '0.022*\"seat\" + 0.014*\"cabin\" + 0.013*\"good\" + 0.009*\"world\" + 0.009*\"crew\" '\n",
      "  '+ 0.008*\"excellent\" + 0.007*\"quality\" + 0.007*\"club\" + 0.007*\"service\" + '\n",
      "  '0.007*\"board\"'),\n",
      " (6,\n",
      "  '0.018*\"crew\" + 0.014*\"cabin\" + 0.012*\"service\" + 0.011*\"board\" + '\n",
      "  '0.010*\"passenger\" + 0.010*\"good\" + 0.010*\"lounge\" + 0.009*\"would\" + '\n",
      "  '0.009*\"seat\" + 0.008*\"offer\"'),\n",
      " (7,\n",
      "  '0.015*\"seat\" + 0.014*\"good\" + 0.011*\"service\" + 0.011*\"cabin\" + '\n",
      "  '0.011*\"food\" + 0.011*\"time\" + 0.009*\"board\" + 0.009*\"one\" + 0.008*\"get\" + '\n",
      "  '0.008*\"crew\"'),\n",
      " (8,\n",
      "  '0.012*\"seat\" + 0.007*\"lounge\" + 0.007*\"concorde\" + 0.007*\"heathrow\" + '\n",
      "  '0.007*\"staff\" + 0.006*\"cabin\" + 0.006*\"cost\" + 0.006*\"little\" + '\n",
      "  '0.006*\"terminal\" + 0.006*\"seem\"'),\n",
      " (9,\n",
      "  '0.011*\"take\" + 0.010*\"fly\" + 0.009*\"crew\" + 0.009*\"excellent\" + '\n",
      "  '0.009*\"champagne\" + 0.008*\"still\" + 0.008*\"seat\" + 0.007*\"time\" + '\n",
      "  '0.007*\"cabin\" + 0.007*\"nice\"'),\n",
      " (10,\n",
      "  '0.017*\"service\" + 0.013*\"lounge\" + 0.012*\"cabin\" + 0.012*\"food\" + '\n",
      "  '0.011*\"new\" + 0.011*\"fast\" + 0.009*\"crew\" + 0.009*\"london\" + 0.009*\"good\" + '\n",
      "  '0.008*\"well\"'),\n",
      " (11,\n",
      "  '0.017*\"food\" + 0.016*\"service\" + 0.013*\"seat\" + 0.011*\"menu\" + 0.010*\"new\" '\n",
      "  '+ 0.009*\"crew\" + 0.009*\"fly\" + 0.008*\"cabin\" + 0.007*\"board\" + 0.007*\"say\"'),\n",
      " (12,\n",
      "  '0.015*\"cabin\" + 0.013*\"food\" + 0.013*\"london\" + 0.012*\"heathrow\" + '\n",
      "  '0.012*\"time\" + 0.012*\"crew\" + 0.010*\"service\" + 0.010*\"seat\" + '\n",
      "  '0.008*\"experience\" + 0.008*\"old\"'),\n",
      " (13,\n",
      "  '0.013*\"seat\" + 0.011*\"wife\" + 0.009*\"call\" + 0.008*\"service\" + 0.008*\"good\" '\n",
      "  '+ 0.008*\"could\" + 0.008*\"phone\" + 0.007*\"help\" + 0.007*\"check\" + '\n",
      "  '0.007*\"nice\"'),\n",
      " (14,\n",
      "  '0.011*\"new\" + 0.008*\"seat\" + 0.008*\"get\" + 0.008*\"champagne\" + '\n",
      "  '0.008*\"sleep\" + 0.005*\"dinner\" + 0.005*\"fine\" + 0.005*\"basic\" + '\n",
      "  '0.005*\"lounge\" + 0.005*\"good\"'),\n",
      " (15,\n",
      "  '0.026*\"cabin\" + 0.021*\"seat\" + 0.014*\"service\" + 0.013*\"good\" + '\n",
      "  '0.012*\"food\" + 0.009*\"passenger\" + 0.008*\"board\" + 0.008*\"crew\" + '\n",
      "  '0.008*\"product\" + 0.008*\"selection\"'),\n",
      " (16,\n",
      "  '0.018*\"food\" + 0.015*\"cabin\" + 0.013*\"good\" + 0.013*\"seat\" + '\n",
      "  '0.009*\"service\" + 0.008*\"crew\" + 0.007*\"singapore\" + 0.007*\"board\" + '\n",
      "  '0.007*\"get\" + 0.007*\"standard\"'),\n",
      " (17,\n",
      "  '0.012*\"hour\" + 0.011*\"bed\" + 0.007*\"nice\" + 0.007*\"one\" + 0.007*\"dont\" + '\n",
      "  '0.007*\"best\" + 0.007*\"seat\" + 0.006*\"well\" + 0.006*\"offer\" + '\n",
      "  '0.006*\"lounge\"'),\n",
      " (18,\n",
      "  '0.018*\"seat\" + 0.009*\"toilet\" + 0.008*\"food\" + 0.008*\"fly\" + 0.007*\"one\" + '\n",
      "  '0.007*\"really\" + 0.007*\"good\" + 0.007*\"back\" + 0.007*\"service\" + '\n",
      "  '0.007*\"crew\"'),\n",
      " (19,\n",
      "  '0.017*\"seat\" + 0.015*\"service\" + 0.013*\"staff\" + 0.012*\"food\" + '\n",
      "  '0.011*\"lounge\" + 0.009*\"one\" + 0.007*\"good\" + 0.007*\"crew\" + 0.007*\"hour\" + '\n",
      "  '0.007*\"passenger\"')]\n"
     ]
    }
   ],
   "source": [
    "# LDA Model for First Class\n",
    "topic_num = 20\n",
    "word_num = 10\n",
    "\n",
    "# Use the tokenized reviews in the CleanText2 column\n",
    "tokenized_reviews_FC = first_class_df['CleanText2']\n",
    "\n",
    "# Creating dictionary and document-term matrix\n",
    "dictionary = gensim.corpora.Dictionary(tokenized_reviews_FC)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_reviews_FC]\n",
    "\n",
    "# Creating the LDA model\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=topic_num, id2word=dictionary, passes=20)\n",
    "\n",
    "# Printing the topics\n",
    "pprint(ldamodel.print_topics(num_topics=topic_num, num_words=word_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314cddc",
   "metadata": {},
   "source": [
    "# 3b) Identifying top occurring trigrams for each Seat Type based on Topic Words identified from Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81fb539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk import trigrams\n",
    "\n",
    "# Generating trigrams as tokenized words (trigrams were initially in object form)\n",
    "def generate_trigrams(df, column_name):\n",
    "    return df[column_name].apply(lambda tokens: list(trigrams(tokens)))\n",
    "\n",
    "# Function to filter trigrams containing a specific word\n",
    "def list_trigrams_containing_word(df, column, word):\n",
    "    all_trigrams = []\n",
    "    for sublist in df[column]:\n",
    "        if isinstance(sublist, list):\n",
    "            for trigram in sublist:\n",
    "                if isinstance(trigram, tuple):\n",
    "                    all_trigrams.append(trigram)\n",
    "    \n",
    "    # Filtering trigrams with specific topic word\n",
    "    filtered_trigrams = [trigram for trigram in all_trigrams if word in trigram]\n",
    "    \n",
    "    return filtered_trigrams\n",
    "\n",
    "\n",
    "# Function to get top 20 trigrams containing the specified word\n",
    "def top_trigrams_with_word(df, column, word, top_n=20):\n",
    "    trigrams_with_word = list_trigrams_containing_word(df, column, word)\n",
    "    trigram_counts = Counter(trigrams_with_word)\n",
    "    return trigram_counts.most_common(top_n)\n",
    "\n",
    "# Function to print top trigrams\n",
    "def print_top_trigrams(trigrams, seat_type, word):\n",
    "    print(f\"Top trigrams featuring '{word}' in reviews of {seat_type}:\")\n",
    "    for trigram, count in trigrams:\n",
    "        print(f\"{trigram}: {count}\")\n",
    "\n",
    "# Print trigrams for each seat type\n",
    "economy_df['Trigrams'] = generate_trigrams(economy_df, 'CleanText2')\n",
    "premium_economy_df['Trigrams'] = generate_trigrams(premium_economy_df, 'CleanText2')\n",
    "business_class_df['Trigrams'] = generate_trigrams(business_class_df, 'CleanText2')\n",
    "first_class_df['Trigrams'] = generate_trigrams(first_class_df, 'CleanText2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a48bd3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top trigrams featuring 'food' in reviews of Economy Class:\n",
      "('charge', 'food', 'drink'): 9\n",
      "('complimentary', 'food', 'drink'): 7\n",
      "('free', 'food', 'drink'): 7\n",
      "('pay', 'food', 'drink'): 7\n",
      "('food', 'drink', 'service'): 7\n",
      "('food', 'pretty', 'good'): 7\n",
      "('purchase', 'food', 'drink'): 5\n",
      "('buy', 'food', 'drink'): 5\n",
      "('food', 'drink', 'serve'): 5\n",
      "('food', 'drink', 'offer'): 4\n",
      "('food', 'beverage', 'board'): 4\n",
      "('entertainment', 'good', 'food'): 4\n",
      "('cabin', 'crew', 'food'): 4\n",
      "('food', 'good', 'service'): 4\n",
      "('food', 'drink', 'short'): 4\n",
      "('food', 'drink', 'good'): 3\n",
      "('crew', 'friendly', 'food'): 3\n",
      "('sell', 'food', 'drink'): 3\n",
      "('food', 'long', 'haul'): 3\n",
      "('start', 'charge', 'food'): 3\n",
      "Top trigrams featuring 'food' in reviews of Premium Economy:\n",
      "('service', 'good', 'food'): 2\n",
      "('crew', 'excellent', 'food'): 2\n",
      "('food', 'inflight', 'entertainment'): 2\n",
      "('entertainment', 'system', 'food'): 2\n",
      "('leg', 'room', 'food'): 2\n",
      "('food', 'wine', 'good'): 2\n",
      "('take', 'food', 'drink'): 2\n",
      "('food', 'choice', 'run'): 2\n",
      "('food', 'quite', 'good'): 2\n",
      "('great', 'food', 'awful'): 2\n",
      "('food', 'drink', 'service'): 2\n",
      "('entertainment', 'good', 'food'): 2\n",
      "('good', 'food', 'good'): 2\n",
      "('cabin', 'staff', 'food'): 2\n",
      "('staff', 'food', 'okay'): 2\n",
      "('food', 'okay', 'memory'): 2\n",
      "('could', 'eat', 'food'): 2\n",
      "('food', 'club', 'world'): 2\n",
      "('staff', 'great', 'food'): 2\n",
      "('comfort', 'special', 'food'): 2\n",
      "Top trigrams featuring 'food' in reviews of Business Class:\n",
      "('service', 'good', 'food'): 10\n",
      "('food', 'drink', 'good'): 9\n",
      "('good', 'selection', 'food'): 7\n",
      "('selection', 'food', 'drink'): 6\n",
      "('drink', 'food', 'service'): 5\n",
      "('club', 'world', 'food'): 5\n",
      "('food', 'drink', 'service'): 5\n",
      "('good', 'choice', 'food'): 5\n",
      "('crew', 'excellent', 'food'): 4\n",
      "('seat', 'comfortable', 'food'): 4\n",
      "('food', 'good', 'wine'): 4\n",
      "('good', 'food', 'drink'): 4\n",
      "('poor', 'quality', 'food'): 4\n",
      "('food', 'quality', 'good'): 4\n",
      "('good', 'quality', 'food'): 3\n",
      "('hot', 'food', 'available'): 3\n",
      "('food', 'quite', 'good'): 3\n",
      "('crew', 'good', 'food'): 3\n",
      "('good', 'food', 'wine'): 3\n",
      "('leave', 'time', 'food'): 3\n",
      "Top trigrams featuring 'food' in reviews of First Class:\n",
      "('good', 'food', 'drink'): 3\n",
      "('food', 'well', 'present'): 3\n",
      "('food', 'drink', 'choice'): 2\n",
      "('food', 'tasty', 'well'): 2\n",
      "('well', 'cook', 'food'): 2\n",
      "('food', 'drink', 'service'): 2\n",
      "('service', 'excellent', 'food'): 2\n",
      "('crew', 'superb', 'food'): 1\n",
      "('superb', 'food', 'beverage'): 1\n",
      "('food', 'beverage', 'find'): 1\n",
      "('friendly', 'respectful', 'food'): 1\n",
      "('respectful', 'food', 'beverage'): 1\n",
      "('food', 'beverage', 'offering'): 1\n",
      "('wine', 'etc', 'food'): 1\n",
      "('etc', 'food', 'bit'): 1\n",
      "('food', 'bit', 'overcooked'): 1\n",
      "('wine', 'menu', 'food'): 1\n",
      "('menu', 'food', 'awful'): 1\n",
      "('food', 'awful', 'steak'): 1\n",
      "('lounge', 'good', 'food'): 1\n"
     ]
    }
   ],
   "source": [
    "# Set the commonly occurring topic word-of-interest\n",
    "word = 'food' \n",
    "\n",
    "# Print top 20 trigrams with count that contain the frequently occurring topic word\n",
    "economy_top_trigrams = top_trigrams_with_word(economy_df, 'Trigrams', word)\n",
    "print_top_trigrams(economy_top_trigrams, \"Economy Class\", word)\n",
    "\n",
    "premium_economy_top_trigrams = top_trigrams_with_word(premium_economy_df, 'Trigrams', word)\n",
    "print_top_trigrams(premium_economy_top_trigrams, \"Premium Economy\", word)\n",
    "\n",
    "business_class_top_trigrams = top_trigrams_with_word(business_class_df, 'Trigrams', word)\n",
    "print_top_trigrams(business_class_top_trigrams, \"Business Class\", word)\n",
    "\n",
    "first_class_top_trigrams = top_trigrams_with_word(first_class_df, 'Trigrams', word)\n",
    "print_top_trigrams(first_class_top_trigrams, \"First Class\", word)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
